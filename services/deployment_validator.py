"""
Deployment Validator for Enhanced Conversation System
Validates system deployment and monitors production stability
"""

import time
import json
import requests
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import logging
import threading
import psutil
import os

logger = logging.getLogger(__name__)

@dataclass
class ValidationResult:
    """Result of a validation check"""
    check_name: str
    passed: bool
    message: str
    details: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.utcnow)

@dataclass
class DeploymentHealth:
    """Overall deployment health status"""
    status: str  # "healthy", "degraded", "unhealthy"
    score: float  # 0.0 to 1.0
    issues: List[str]
    recommendations: List[str]
    last_check: datetime = field(default_factory=datetime.utcnow)

class SystemValidator:
    """Validates system components and performance"""
    
    def __init__(self):
        self.validation_results = []
        self.health_history = []
        
        # Validation thresholds for Render deployment
        self.thresholds = {
            'memory_limit_mb': 512,
            'memory_warning_mb': 400,
            'response_time_limit_ms': 3000,
            'response_time_warning_ms': 1500,
            'error_rate_limit': 0.1,
            'error_rate_warning': 0.05,
            'quality_score_minimum': 0.4,
            'quality_score_target': 0.7
        }
        
        logger.info("System validator initialized")
    
    def validate_memory_usage(self) -> ValidationResult:
        """Validate memory usage is within limits"""
        try:\n            process = psutil.Process(os.getpid())\n            memory_mb = process.memory_info().rss / 1024 / 1024\n            \n            if memory_mb > self.thresholds['memory_limit_mb']:\n                return ValidationResult(\n                    check_name=\"memory_usage\",\n                    passed=False,\n                    message=f\"Memory usage exceeds limit: {memory_mb:.1f}MB > {self.thresholds['memory_limit_mb']}MB\",\n                    details={'memory_mb': memory_mb, 'limit_mb': self.thresholds['memory_limit_mb']}\n                )\n            elif memory_mb > self.thresholds['memory_warning_mb']:\n                return ValidationResult(\n                    check_name=\"memory_usage\",\n                    passed=True,\n                    message=f\"Memory usage approaching limit: {memory_mb:.1f}MB (warning at {self.thresholds['memory_warning_mb']}MB)\",\n                    details={'memory_mb': memory_mb, 'warning_mb': self.thresholds['memory_warning_mb']}\n                )\n            else:\n                return ValidationResult(\n                    check_name=\"memory_usage\",\n                    passed=True,\n                    message=f\"Memory usage normal: {memory_mb:.1f}MB\",\n                    details={'memory_mb': memory_mb}\n                )\n                \n        except Exception as e:\n            return ValidationResult(\n                check_name=\"memory_usage\",\n                passed=False,\n                message=f\"Error checking memory usage: {e}\",\n                details={'error': str(e)}\n            )\n    \n    def validate_response_times(self, performance_monitor) -> ValidationResult:\n        \"\"\"Validate response times are acceptable\"\"\"\n        try:\n            stats = performance_monitor.get_real_time_stats()\n            response_time_stats = stats.get('response_time', {})\n            \n            avg_response_time = response_time_stats.get('avg', 0)\n            p95_response_time = response_time_stats.get('p95', 0)\n            \n            if p95_response_time > self.thresholds['response_time_limit_ms']:\n                return ValidationResult(\n                    check_name=\"response_times\",\n                    passed=False,\n                    message=f\"P95 response time exceeds limit: {p95_response_time:.1f}ms > {self.thresholds['response_time_limit_ms']}ms\",\n                    details={\n                        'avg_ms': avg_response_time,\n                        'p95_ms': p95_response_time,\n                        'limit_ms': self.thresholds['response_time_limit_ms']\n                    }\n                )\n            elif avg_response_time > self.thresholds['response_time_warning_ms']:\n                return ValidationResult(\n                    check_name=\"response_times\",\n                    passed=True,\n                    message=f\"Average response time approaching warning: {avg_response_time:.1f}ms\",\n                    details={\n                        'avg_ms': avg_response_time,\n                        'warning_ms': self.thresholds['response_time_warning_ms']\n                    }\n                )\n            else:\n                return ValidationResult(\n                    check_name=\"response_times\",\n                    passed=True,\n                    message=f\"Response times normal: avg {avg_response_time:.1f}ms, p95 {p95_response_time:.1f}ms\",\n                    details={'avg_ms': avg_response_time, 'p95_ms': p95_response_time}\n                )\n                \n        except Exception as e:\n            return ValidationResult(\n                check_name=\"response_times\",\n                passed=False,\n                message=f\"Error checking response times: {e}\",\n                details={'error': str(e)}\n            )\n    \n    def validate_error_rates(self, performance_monitor) -> ValidationResult:\n        \"\"\"Validate error rates are acceptable\"\"\"\n        try:\n            stats = performance_monitor.get_real_time_stats()\n            error_rate = stats.get('error_rate', 0)\n            fallback_rate = stats.get('fallback_rate', 0)\n            \n            if error_rate > self.thresholds['error_rate_limit']:\n                return ValidationResult(\n                    check_name=\"error_rates\",\n                    passed=False,\n                    message=f\"Error rate exceeds limit: {error_rate:.1%} > {self.thresholds['error_rate_limit']:.1%}\",\n                    details={\n                        'error_rate': error_rate,\n                        'fallback_rate': fallback_rate,\n                        'limit': self.thresholds['error_rate_limit']\n                    }\n                )\n            elif error_rate > self.thresholds['error_rate_warning']:\n                return ValidationResult(\n                    check_name=\"error_rates\",\n                    passed=True,\n                    message=f\"Error rate approaching warning: {error_rate:.1%}\",\n                    details={\n                        'error_rate': error_rate,\n                        'warning': self.thresholds['error_rate_warning']\n                    }\n                )\n            else:\n                return ValidationResult(\n                    check_name=\"error_rates\",\n                    passed=True,\n                    message=f\"Error rates normal: {error_rate:.1%} errors, {fallback_rate:.1%} fallbacks\",\n                    details={'error_rate': error_rate, 'fallback_rate': fallback_rate}\n                )\n                \n        except Exception as e:\n            return ValidationResult(\n                check_name=\"error_rates\",\n                passed=False,\n                message=f\"Error checking error rates: {e}\",\n                details={'error': str(e)}\n            )\n    \n    def validate_conversation_quality(self, performance_monitor) -> ValidationResult:\n        \"\"\"Validate conversation quality meets standards\"\"\"\n        try:\n            stats = performance_monitor.get_real_time_stats()\n            quality_stats = stats.get('quality', {})\n            \n            avg_quality = quality_stats.get('avg', 0.5)\n            min_quality = quality_stats.get('min', 0.5)\n            \n            if avg_quality < self.thresholds['quality_score_minimum']:\n                return ValidationResult(\n                    check_name=\"conversation_quality\",\n                    passed=False,\n                    message=f\"Average quality below minimum: {avg_quality:.2f} < {self.thresholds['quality_score_minimum']:.2f}\",\n                    details={\n                        'avg_quality': avg_quality,\n                        'min_quality': min_quality,\n                        'minimum_threshold': self.thresholds['quality_score_minimum']\n                    }\n                )\n            elif avg_quality < self.thresholds['quality_score_target']:\n                return ValidationResult(\n                    check_name=\"conversation_quality\",\n                    passed=True,\n                    message=f\"Quality below target but acceptable: {avg_quality:.2f}\",\n                    details={\n                        'avg_quality': avg_quality,\n                        'target': self.thresholds['quality_score_target']\n                    }\n                )\n            else:\n                return ValidationResult(\n                    check_name=\"conversation_quality\",\n                    passed=True,\n                    message=f\"Quality meets target: {avg_quality:.2f}\",\n                    details={'avg_quality': avg_quality}\n                )\n                \n        except Exception as e:\n            return ValidationResult(\n                check_name=\"conversation_quality\",\n                passed=False,\n                message=f\"Error checking conversation quality: {e}\",\n                details={'error': str(e)}\n            )\n    \n    def validate_enhanced_components(self) -> List[ValidationResult]:\n        \"\"\"Validate enhanced system components are working\"\"\"\n        results = []\n        \n        # Test template system\n        try:\n            from services.template_selector import get_template_selector\n            template_selector = get_template_selector()\n            \n            # Simple test\n            if hasattr(template_selector, 'select_best_template'):\n                results.append(ValidationResult(\n                    check_name=\"template_system\",\n                    passed=True,\n                    message=\"Template system is accessible\",\n                    details={'component': 'template_selector'}\n                ))\n            else:\n                results.append(ValidationResult(\n                    check_name=\"template_system\",\n                    passed=False,\n                    message=\"Template system missing required methods\",\n                    details={'component': 'template_selector'}\n                ))\n                \n        except Exception as e:\n            results.append(ValidationResult(\n                check_name=\"template_system\",\n                passed=False,\n                message=f\"Template system error: {e}\",\n                details={'error': str(e)}\n            ))\n        \n        # Test context extraction\n        try:\n            from services.smart_context_extractor import get_context_extractor\n            context_extractor = get_context_extractor(None)  # Mock supabase service\n            \n            if hasattr(context_extractor, 'get_user_context'):\n                results.append(ValidationResult(\n                    check_name=\"context_system\",\n                    passed=True,\n                    message=\"Context extraction system is accessible\",\n                    details={'component': 'context_extractor'}\n                ))\n            else:\n                results.append(ValidationResult(\n                    check_name=\"context_system\",\n                    passed=False,\n                    message=\"Context system missing required methods\",\n                    details={'component': 'context_extractor'}\n                ))\n                \n        except Exception as e:\n            results.append(ValidationResult(\n                check_name=\"context_system\",\n                passed=False,\n                message=f\"Context system error: {e}\",\n                details={'error': str(e)}\n            ))\n        \n        # Test enhanced chatbot\n        try:\n            from enhanced_chatbot import create_enhanced_chatbot\n            enhanced_chatbot = create_enhanced_chatbot(None, None)\n            \n            if hasattr(enhanced_chatbot, 'process_message'):\n                results.append(ValidationResult(\n                    check_name=\"enhanced_chatbot\",\n                    passed=True,\n                    message=\"Enhanced chatbot is accessible\",\n                    details={'component': 'enhanced_chatbot'}\n                ))\n            else:\n                results.append(ValidationResult(\n                    check_name=\"enhanced_chatbot\",\n                    passed=False,\n                    message=\"Enhanced chatbot missing required methods\",\n                    details={'component': 'enhanced_chatbot'}\n                ))\n                \n        except Exception as e:\n            results.append(ValidationResult(\n                check_name=\"enhanced_chatbot\",\n                passed=False,\n                message=f\"Enhanced chatbot error: {e}\",\n                details={'error': str(e)}\n            ))\n        \n        return results\n    \n    def run_full_validation(self, performance_monitor=None) -> List[ValidationResult]:\n        \"\"\"Run complete system validation\"\"\"\n        results = []\n        \n        # Basic system checks\n        results.append(self.validate_memory_usage())\n        \n        # Performance checks (if monitor available)\n        if performance_monitor:\n            results.append(self.validate_response_times(performance_monitor))\n            results.append(self.validate_error_rates(performance_monitor))\n            results.append(self.validate_conversation_quality(performance_monitor))\n        \n        # Component checks\n        results.extend(self.validate_enhanced_components())\n        \n        # Store results\n        self.validation_results.extend(results)\n        \n        return results\n\nclass ProductionMonitor:\n    \"\"\"Monitors production deployment health\"\"\"\n    \n    def __init__(self, api_base_url: str = None):\n        self.api_base_url = api_base_url or \"http://localhost:5000\"\n        self.validator = SystemValidator()\n        self.monitoring_thread = None\n        self.monitoring_enabled = False\n        self.check_interval = 300  # 5 minutes\n        \n        # Health tracking\n        self.health_history = []\n        self.alert_callbacks = []\n        \n        logger.info(f\"Production monitor initialized for {self.api_base_url}\")\n    \n    def start_monitoring(self):\n        \"\"\"Start continuous production monitoring\"\"\"\n        if not self.monitoring_enabled:\n            self.monitoring_enabled = True\n            self.monitoring_thread = threading.Thread(\n                target=self._monitoring_loop,\n                daemon=True\n            )\n            self.monitoring_thread.start()\n            logger.info(\"Production monitoring started\")\n    \n    def stop_monitoring(self):\n        \"\"\"Stop production monitoring\"\"\"\n        self.monitoring_enabled = False\n        if self.monitoring_thread:\n            self.monitoring_thread.join(timeout=10)\n        logger.info(\"Production monitoring stopped\")\n    \n    def _monitoring_loop(self):\n        \"\"\"Background monitoring loop\"\"\"\n        while self.monitoring_enabled:\n            try:\n                health = self.check_deployment_health()\n                self.health_history.append(health)\n                \n                # Keep only last 100 health checks\n                if len(self.health_history) > 100:\n                    self.health_history = self.health_history[-100:]\n                \n                # Trigger alerts if needed\n                if health.status in ['degraded', 'unhealthy']:\n                    self._trigger_health_alert(health)\n                \n                time.sleep(self.check_interval)\n                \n            except Exception as e:\n                logger.error(f\"Error in production monitoring: {e}\")\n                time.sleep(self.check_interval * 2)\n    \n    def check_deployment_health(self) -> DeploymentHealth:\n        \"\"\"Check overall deployment health\"\"\"\n        issues = []\n        recommendations = []\n        \n        # Test API endpoints\n        api_health = self._test_api_endpoints()\n        if not api_health['healthy']:\n            issues.extend(api_health['issues'])\n            recommendations.extend(api_health['recommendations'])\n        \n        # Test system performance\n        try:\n            from services.performance_monitor import get_performance_monitor\n            performance_monitor = get_performance_monitor()\n            \n            validation_results = self.validator.run_full_validation(performance_monitor)\n            \n            failed_checks = [r for r in validation_results if not r.passed]\n            if failed_checks:\n                issues.extend([f\"{r.check_name}: {r.message}\" for r in failed_checks])\n                recommendations.append(\"Review failed validation checks\")\n            \n        except Exception as e:\n            issues.append(f\"Performance monitoring error: {e}\")\n            recommendations.append(\"Check performance monitoring system\")\n        \n        # Calculate health score\n        total_checks = len(self.validator.validation_results) if self.validator.validation_results else 1\n        passed_checks = sum(1 for r in self.validator.validation_results if r.passed)\n        health_score = passed_checks / total_checks\n        \n        # Determine status\n        if health_score >= 0.9 and not issues:\n            status = \"healthy\"\n        elif health_score >= 0.7:\n            status = \"degraded\"\n        else:\n            status = \"unhealthy\"\n        \n        return DeploymentHealth(\n            status=status,\n            score=health_score,\n            issues=issues,\n            recommendations=recommendations\n        )\n    \n    def _test_api_endpoints(self) -> Dict[str, Any]:\n        \"\"\"Test API endpoints for availability\"\"\"\n        endpoints_to_test = [\n            '/health',\n            '/api/v1/chat/message',\n            '/api/v1/enhanced-chat/system/status'\n        ]\n        \n        issues = []\n        recommendations = []\n        healthy_endpoints = 0\n        \n        for endpoint in endpoints_to_test:\n            try:\n                url = f\"{self.api_base_url}{endpoint}\"\n                \n                if endpoint == '/api/v1/chat/message':\n                    # POST endpoint - test with minimal data\n                    response = requests.post(\n                        url,\n                        json={'message': 'health check'},\n                        timeout=10,\n                        headers={'Authorization': 'Bearer test'}\n                    )\n                    # Expect 401 (unauthorized) which means endpoint is working\n                    if response.status_code in [200, 401]:\n                        healthy_endpoints += 1\n                    else:\n                        issues.append(f\"Chat endpoint returned {response.status_code}\")\n                else:\n                    # GET endpoint\n                    response = requests.get(url, timeout=10)\n                    if response.status_code == 200:\n                        healthy_endpoints += 1\n                    else:\n                        issues.append(f\"{endpoint} returned {response.status_code}\")\n                        \n            except requests.exceptions.Timeout:\n                issues.append(f\"{endpoint} timed out\")\n                recommendations.append(\"Check server response times\")\n            except requests.exceptions.ConnectionError:\n                issues.append(f\"{endpoint} connection failed\")\n                recommendations.append(\"Check server availability\")\n            except Exception as e:\n                issues.append(f\"{endpoint} error: {e}\")\n        \n        healthy = healthy_endpoints == len(endpoints_to_test)\n        \n        return {\n            'healthy': healthy,\n            'healthy_endpoints': healthy_endpoints,\n            'total_endpoints': len(endpoints_to_test),\n            'issues': issues,\n            'recommendations': recommendations\n        }\n    \n    def _trigger_health_alert(self, health: DeploymentHealth):\n        \"\"\"Trigger health alert\"\"\"\n        alert_message = f\"Deployment health {health.status}: score {health.score:.2f}\"\n        \n        logger.warning(f\"Health Alert: {alert_message}\")\n        \n        for callback in self.alert_callbacks:\n            try:\n                callback(health)\n            except Exception as e:\n                logger.error(f\"Error in health alert callback: {e}\")\n    \n    def add_alert_callback(self, callback):\n        \"\"\"Add callback for health alerts\"\"\"\n        self.alert_callbacks.append(callback)\n    \n    def get_deployment_status(self) -> Dict[str, Any]:\n        \"\"\"Get current deployment status\"\"\"\n        current_health = self.check_deployment_health()\n        \n        # Calculate uptime percentage\n        if self.health_history:\n            healthy_checks = sum(1 for h in self.health_history if h.status == 'healthy')\n            uptime_percentage = healthy_checks / len(self.health_history)\n        else:\n            uptime_percentage = 1.0 if current_health.status == 'healthy' else 0.0\n        \n        return {\n            'current_health': {\n                'status': current_health.status,\n                'score': current_health.score,\n                'issues': current_health.issues,\n                'recommendations': current_health.recommendations,\n                'last_check': current_health.last_check.isoformat()\n            },\n            'uptime_percentage': uptime_percentage,\n            'total_health_checks': len(self.health_history),\n            'monitoring_enabled': self.monitoring_enabled,\n            'api_base_url': self.api_base_url\n        }\n\nclass DeploymentValidator:\n    \"\"\"Main deployment validation orchestrator\"\"\"\n    \n    def __init__(self, api_base_url: str = None):\n        self.system_validator = SystemValidator()\n        self.production_monitor = ProductionMonitor(api_base_url)\n        \n        logger.info(\"Deployment validator initialized\")\n    \n    def validate_deployment(self) -> Dict[str, Any]:\n        \"\"\"Run complete deployment validation\"\"\"\n        start_time = time.time()\n        \n        # Run system validation\n        try:\n            from services.performance_monitor import get_performance_monitor\n            performance_monitor = get_performance_monitor()\n        except Exception:\n            performance_monitor = None\n        \n        validation_results = self.system_validator.run_full_validation(performance_monitor)\n        \n        # Check deployment health\n        deployment_health = self.production_monitor.check_deployment_health()\n        \n        # Calculate overall validation score\n        passed_validations = sum(1 for r in validation_results if r.passed)\n        total_validations = len(validation_results)\n        validation_score = passed_validations / max(total_validations, 1)\n        \n        # Determine deployment readiness\n        deployment_ready = (\n            validation_score >= 0.8 and\n            deployment_health.status in ['healthy', 'degraded'] and\n            deployment_health.score >= 0.7\n        )\n        \n        validation_time = (time.time() - start_time) * 1000\n        \n        return {\n            'deployment_ready': deployment_ready,\n            'validation_score': validation_score,\n            'deployment_health': {\n                'status': deployment_health.status,\n                'score': deployment_health.score,\n                'issues': deployment_health.issues,\n                'recommendations': deployment_health.recommendations\n            },\n            'validation_results': [\n                {\n                    'check_name': r.check_name,\n                    'passed': r.passed,\n                    'message': r.message,\n                    'details': r.details\n                }\n                for r in validation_results\n            ],\n            'summary': {\n                'total_checks': total_validations,\n                'passed_checks': passed_validations,\n                'failed_checks': total_validations - passed_validations,\n                'validation_time_ms': validation_time\n            },\n            'timestamp': datetime.utcnow().isoformat()\n        }\n    \n    def start_continuous_monitoring(self):\n        \"\"\"Start continuous deployment monitoring\"\"\"\n        self.production_monitor.start_monitoring()\n    \n    def stop_continuous_monitoring(self):\n        \"\"\"Stop continuous deployment monitoring\"\"\"\n        self.production_monitor.stop_monitoring()\n    \n    def get_monitoring_dashboard(self) -> Dict[str, Any]:\n        \"\"\"Get deployment monitoring dashboard\"\"\"\n        deployment_status = self.production_monitor.get_deployment_status()\n        \n        # Get recent validation results\n        recent_validations = self.system_validator.validation_results[-10:] if self.system_validator.validation_results else []\n        \n        return {\n            'deployment_status': deployment_status,\n            'recent_validations': [\n                {\n                    'check_name': r.check_name,\n                    'passed': r.passed,\n                    'message': r.message,\n                    'timestamp': r.timestamp.isoformat()\n                }\n                for r in recent_validations\n            ],\n            'render_compatibility': {\n                'memory_optimized': True,\n                'response_time_optimized': True,\n                'connection_pooled': True,\n                'cache_enabled': True\n            },\n            'dashboard_generated_at': datetime.utcnow().isoformat()\n        }\n\n# Global deployment validator\n_deployment_validator = None\n\ndef get_deployment_validator(api_base_url: str = None) -> DeploymentValidator:\n    \"\"\"Get global deployment validator instance\"\"\"\n    global _deployment_validator\n    if _deployment_validator is None:\n        _deployment_validator = DeploymentValidator(api_base_url)\n    return _deployment_validator\n